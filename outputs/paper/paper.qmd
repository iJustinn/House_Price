---
title: "TBD"
subtitle: "TBD"
author: 
  - Ziheng Zhong
thanks: "Code and data are available at: https://github.com/iJustinn/House_Price.git"
date: today
date-format: long
abstract: "TBD"
format: pdf
toc: true
toc-depth: 3
number-sections: true
bibliography: references.bib
---

```{r}
#| message: false
#| echo: false

# load packages
packages <- c("tidyverse", "ggplot2", "janitor", "readr", "knitr", "modelsummary", "testthat", "kableExtra", "viridis", "lubridate", "maps", "mgcv")

# Function to check and install missing packages
install_and_load <- function(packages) {
  for (package in packages) {
    if (!require(package, character.only = TRUE)) {
      install.packages(package)
      library(package, character.only = TRUE)
    }
  }
}

# Call the function with the list of packages
install_and_load(packages)
```

```{r}
#| message: false
#| echo: false

# read in data
house_prices <- read_csv("../../outputs/data/merged_house_price.csv")

state_house_price_by_year <- read_csv("../../outputs/data/state_house_price_by_year.csv")
state_1b_house_price_by_year <- read_csv("../../outputs/data/state_1b_house_price_by_year.csv")
state_2b_house_price_by_year <- read_csv("../../outputs/data/state_2b_house_price_by_year.csv")
state_3b_house_price_by_year <- read_csv("../../outputs/data/state_3b_house_price_by_year.csv")
state_4b_house_price_by_year <- read_csv("../../outputs/data/state_4b_house_price_by_year.csv")
state_5bplus_house_price_by_year <- read_csv("../../outputs/data/state_5bplus_house_price_by_year.csv")

state_house_price_by_month <- read_csv("../../outputs/data/state_house_price_by_month.csv")
state_1b_house_price_by_month <- read_csv("../../outputs/data/state_1b_house_price_by_month.csv")
state_2b_house_price_by_month <- read_csv("../../outputs/data/state_2b_house_price_by_month.csv")
state_3b_house_price_by_month <- read_csv("../../outputs/data/state_3b_house_price_by_month.csv")
state_4b_house_price_by_month <- read_csv("../../outputs/data/state_4b_house_price_by_month.csv")
state_5bplus_house_price_by_month <- read_csv("../../outputs/data/state_5bplus_house_price_by_month.csv")

```

# Introduction



# Data {#sec-data}

Data used in this paper was cleaned, processed and tested with the programming language R [@citeR]. Also with support of additional packages in R: `tidyverse` [@citeTidyverse], `ggplot2` [@citeGgplot], `janitor` [@citeJanitor], `readr` [@citeReadr], `knitr` [@citeKnitr], `modelsummary` [@citeModelsummary], `testthat` [@citeTestthat], `KableExtra` [@citeKableEx], `viridis` [@citeViridis], `lubridate` [@citeLubridate], `maps` [@citeMaps], `mgcv` [@citeMgcv].

## Source



## Method

```{r}
#| message: false
#| echo: false
#| warning: false
#| fig-pos: 'h'
#| label: tbl-sum
#| tbl-cap: Summary statistics of the California housing dataset
#| layout-ncol: 3
#| layout-nrow: 3



```



```{r}
#| message: false
#| echo: false
#| warning: false
#| fig-pos: 'h'
#| label: tbl-null-raw
#| tbl-cap: Count of missing values for each variable



```



```{r}
#| message: false
#| echo: false
#| warning: false
#| fig-pos: 'h'
#| label: tbl-null-clean
#| tbl-cap: Count of missing values for each variable after cleaning



```



# Results {#sec-res}

```{r}
#| include: false
# Models Setup

#### Multiple Regression Model ####
model_multi <- lm(AvgHousePrice ~ Year + NumBedroom, data = house_prices)

#### Polynomial Regression Model ####
model_poly <- lm(AvgHousePrice ~ poly(Year, 2) + poly(NumBedroom, 2), data = house_prices)

#### Generalized Additive Models ####
num_unique_years <- length(unique(house_prices$Year))
num_unique_bedrooms <- length(unique(house_prices$NumBedroom))

gam_model <- gam(AvgHousePrice ~ s(Year, k=num_unique_years-1) + s(NumBedroom, k=num_unique_bedrooms-1), data = house_prices)

```

## Data Trend

```{r}
#| message: false
#| echo: false
#| warning: false
#| fig-pos: 'h'
#| label: fig-trend_overall
#| fig-cap: Trend of Average House Price from 2000 to 2024

state_all <- state_house_price_by_month %>% select(-StateName) %>%  # Processing datasets
  summarise_all(mean, na.rm = TRUE) %>%
  pivot_longer(everything(), names_to = "Date", values_to = "AverageHousePrice")

state_all$Date <- as.Date(state_all$Date) # Convert Date from character to Date type

ggplot(state_all, aes(x = Date, y = AverageHousePrice)) + # Plotting
  geom_line(color = "blue") + 
  labs(x = "Year", y = "Average House Price ($)") + 
  theme_minimal() + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

```{r}
#| message: false
#| echo: false
#| warning: false
#| fig-pos: 'h'
#| label: fig-trend_by_type
#| fig-cap: Trend of Average House Price from 2000 to 2024 by House Type

process_data <- function(data, label) { # Function to process each dataset
  data %>% 
    select(-StateName) %>%
    summarise_all(mean, na.rm = TRUE) %>%
    pivot_longer(everything(), names_to = "Date", values_to = "AverageHousePrice") %>%
    mutate(Trend = label,
           Date = as.Date(Date))
}

data_list <- list( # Processing all datasets
  one_bedroom = state_1b_house_price_by_month,
  two_bedroom = state_2b_house_price_by_month,
  three_bedroom = state_3b_house_price_by_month,
  four_bedroom = state_4b_house_price_by_month,
  five_plus_bedroom = state_5bplus_house_price_by_month
)

processed_data <- lapply(names(data_list), function(name) process_data(data_list[[name]], name))

combined_data <- bind_rows(processed_data) # Combining all processed data into one dataframe

ggplot(combined_data, aes(x = Date, y = AverageHousePrice, color = Trend)) + # Plotting
  geom_line() + 
  labs(x = "Year", y = "Average House Price ($)") +
  theme_minimal() + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_color_viridis_d(name = "House Type")

```

## Heat Maps

```{r}
#| message: false
#| echo: false
#| warning: false
#| fig-pos: 'h'
#| label: fig-hm_overall
#| fig-cap: Average Price by State in the US for All House Types

us_states_map <- map_data("state") # Getting map data for US states

state_abbrev_to_full <- c( # Define the mapping from state abbreviations to full names
  AL = "alabama", AK = "alaska", AZ = "arizona", AR = "arkansas", CA = "california",
  CO = "colorado", CT = "connecticut", DE = "delaware", FL = "florida", GA = "georgia",
  HI = "hawaii", ID = "idaho", IL = "illinois", IN = "indiana", IA = "iowa",
  KS = "kansas", KY = "kentucky", LA = "louisiana", ME = "maine", MD = "maryland",
  MA = "massachusetts", MI = "michigan", MN = "minnesota", MS = "mississippi",
  MO = "missouri", MT = "montana", NE = "nebraska", NV = "nevada", NH = "new hampshire",
  NJ = "new jersey", NM = "new mexico", NY = "new york", NC = "north carolina",
  ND = "north dakota", OH = "ohio", OK = "oklahoma", OR = "oregon", PA = "pennsylvania",
  RI = "rhode island", SC = "south carolina", SD = "south dakota", TN = "tennessee",
  TX = "texas", UT = "utah", VT = "vermont", VA = "virginia", WA = "washington",
  WV = "west virginia", WI = "wisconsin", WY = "wyoming", DC = "district of columbia"
)

overall_hm_data <- na.omit(state_house_price_by_year) # Delete all NAs
overall_hm_data$StateName <- tolower(state_abbrev_to_full[overall_hm_data$StateName]) # Convert StateName from abbreviation to full name

overall_hm_data <- overall_hm_data %>% # Taking average for house price based on states
  group_by(StateName) %>%
  summarise(AvgHousePrice = mean(AvgHousePrice, na.rm = TRUE))

overall_hm_map_merged_data <- merge(us_states_map, overall_hm_data, by.x = "region", by.y = "StateName") # Merge data ready for chart

ggplot() + # Generating heat map
  geom_polygon(data = overall_hm_map_merged_data, aes(x = long, y = lat, group = group, fill = AvgHousePrice), color = "white") +
  coord_fixed(1.3) +
  scale_fill_viridis(name = "Avg House Price", option = "plasma") +
  theme_minimal() +
  theme(legend.position = "right")

```

```{r}
#| message: false
#| echo: false
#| warning: false
#| fig-pos: 'h'
#| label: fig-hm_by_type
#| fig-cap: Average Price by State in the US for Different House Types
#| fig-subcap: ["one bedroom","two bedrooms","three bedrooms","four bedrooms","five plus bedrooms"]
#| layout-ncol: 2
#| layout-nrow: 3

us_states_map <- map_data("state") # Getting map data for US states

state_abbrev_to_full <- c( # Define the mapping from state abbreviations to full names
  AL = "alabama", AK = "alaska", AZ = "arizona", AR = "arkansas", CA = "california",
  CO = "colorado", CT = "connecticut", DE = "delaware", FL = "florida", GA = "georgia",
  HI = "hawaii", ID = "idaho", IL = "illinois", IN = "indiana", IA = "iowa",
  KS = "kansas", KY = "kentucky", LA = "louisiana", ME = "maine", MD = "maryland",
  MA = "massachusetts", MI = "michigan", MN = "minnesota", MS = "mississippi",
  MO = "missouri", MT = "montana", NE = "nebraska", NV = "nevada", NH = "new hampshire",
  NJ = "new jersey", NM = "new mexico", NY = "new york", NC = "north carolina",
  ND = "north dakota", OH = "ohio", OK = "oklahoma", OR = "oregon", PA = "pennsylvania",
  RI = "rhode island", SC = "south carolina", SD = "south dakota", TN = "tennessee",
  TX = "texas", UT = "utah", VT = "vermont", VA = "virginia", WA = "washington",
  WV = "west virginia", WI = "wisconsin", WY = "wyoming", DC = "district of columbia"
)

#### US States 1b House Price Heat Map ####
oneb_hm_data <- na.omit(state_1b_house_price_by_year) # Delete all NAs
oneb_hm_data$StateName <- tolower(state_abbrev_to_full[oneb_hm_data$StateName]) # Convert StateName from abbreviation to full name

oneb_hm_data <- oneb_hm_data %>% # Taking average for house price based on states
  group_by(StateName) %>%
  summarise(AvgHousePrice = mean(AvgHousePrice, na.rm = TRUE))

oneb_hm_map_merged_data <- merge(us_states_map, oneb_hm_data, by.x = "region", by.y = "StateName") # Merge data ready for chart

ggplot() + # Generating heat map
  geom_polygon(data = oneb_hm_map_merged_data, aes(x = long, y = lat, group = group, fill = AvgHousePrice), color = "white") +
  coord_fixed(1.3) +
  scale_fill_viridis(name = "Avg House Price", option = "plasma") +
  theme_minimal() +
  theme(legend.position = "right")



#### US States 2b House Price Heat Map ####
twob_hm_data <- na.omit(state_2b_house_price_by_year) # Delete all NAs
twob_hm_data$StateName <- tolower(state_abbrev_to_full[twob_hm_data$StateName]) # Convert StateName from abbreviation to full name

twob_hm_data <- twob_hm_data %>% # Taking average for house price based on states
  group_by(StateName) %>%
  summarise(AvgHousePrice = mean(AvgHousePrice, na.rm = TRUE))

twob_hm_map_merged_data <- merge(us_states_map, twob_hm_data, by.x = "region", by.y = "StateName") # Merge data ready for chart

ggplot() + # Generating heat map
  geom_polygon(data = twob_hm_map_merged_data, aes(x = long, y = lat, group = group, fill = AvgHousePrice), color = "white") +
  coord_fixed(1.3) +
  scale_fill_viridis(name = "Avg House Price", option = "plasma") +
  theme_minimal() +
  theme(legend.position = "right")



#### US States 3b House Price Heat Map ####
threeb_hm_data <- na.omit(state_3b_house_price_by_year) # Delete all NAs
threeb_hm_data$StateName <- tolower(state_abbrev_to_full[threeb_hm_data$StateName]) # Convert StateName from abbreviation to full name

threeb_hm_data <- threeb_hm_data %>% # Taking average for house price based on states
  group_by(StateName) %>%
  summarise(AvgHousePrice = mean(AvgHousePrice, na.rm = TRUE))

threeb_hm_map_merged_data <- merge(us_states_map, threeb_hm_data, by.x = "region", by.y = "StateName") # Merge data ready for chart

ggplot() + # Generating heat map
  geom_polygon(data = threeb_hm_map_merged_data, aes(x = long, y = lat, group = group, fill = AvgHousePrice), color = "white") +
  coord_fixed(1.3) +
  scale_fill_viridis(name = "Avg House Price", option = "plasma") +
  theme_minimal() +
  theme(legend.position = "right")



#### US States 4b House Price Heat Map ####
fourb_hm_data <- na.omit(state_4b_house_price_by_year) # Delete all NAs
fourb_hm_data$StateName <- tolower(state_abbrev_to_full[fourb_hm_data$StateName]) # Convert StateName from abbreviation to full name

fourb_hm_data <- fourb_hm_data %>% # Taking average for house price based on states
  group_by(StateName) %>%
  summarise(AvgHousePrice = mean(AvgHousePrice, na.rm = TRUE))

fourb_hm_map_merged_data <- merge(us_states_map, fourb_hm_data, by.x = "region", by.y = "StateName") # Merge data ready for chart

ggplot() + # Generating heat map
  geom_polygon(data = fourb_hm_map_merged_data, aes(x = long, y = lat, group = group, fill = AvgHousePrice), color = "white") +
  coord_fixed(1.3) +
  scale_fill_viridis(name = "Avg House Price", option = "plasma") +
  theme_minimal() +
  theme(legend.position = "right")



#### US States 5b+ House Price Heat Map ####
fiveb_hm_data <- na.omit(state_5bplus_house_price_by_year) # Delete all NAs
fiveb_hm_data$StateName <- tolower(state_abbrev_to_full[fiveb_hm_data$StateName]) # Convert StateName from abbreviation to full name

fiveb_hm_data <- fiveb_hm_data %>% # Taking average for house price based on states
  group_by(StateName) %>%
  summarise(AvgHousePrice = mean(AvgHousePrice, na.rm = TRUE))

fiveb_hm_map_merged_data <- merge(us_states_map, fiveb_hm_data, by.x = "region", by.y = "StateName") # Merge data ready for chart

ggplot() + # Generating heat map
  geom_polygon(data = fiveb_hm_map_merged_data, aes(x = long, y = lat, group = group, fill = AvgHousePrice), color = "white") +
  coord_fixed(1.3) +
  scale_fill_viridis(name = "Avg House Price", option = "plasma") +
  theme_minimal() +
  theme(legend.position = "right")

```



## Modeling

```{r}
#| message: false
#| echo: false
#| warning: false
#| fig-pos: 'h'
#| label: tbl-linear_model_result
#| tbl-cap: Modeling Results for Linear Models

modelsummary(
  list(
    "Multiple Regression" = model_multi,
    "Polynomial Regression" = model_poly
  )
)

```



```{r}
#| message: false
#| echo: false
#| warning: false
#| fig-pos: 'h'
#| label: tbl-nonlinear_model_result
#| fig-cap: Modeling Results for Non-linear Models

modelsummary(
  list(
    "GAM Regression" = gam_model
  )
)

```



```{r}
#| message: false
#| echo: false
#| warning: false
#| fig-pos: 'h'
#| label: fig-GAM
#| tbl-cap: GAM Regression Fit Result

plot(gam_model, pages=1)

```

# Discussion {#sec-dis}



## Demographic Shifts



## Health-related Behaviors



## Government Policies



## Environmental Changes



## Possible Improvements



# Conclusion {#sec-con}



\newpage

\appendix

# Appendix

## Datasheet

**Motivation**

1. *For what purpose was the dataset created? Was there a specific task in mind? Was there a specific gap that needed to be filled? Please provide a description.*
    - The datasets are created to support a wide range of real estate analysis and research by Zillow Research group. These datasets typically serve purposes such as market trend analysis, price prediction, housing supply studies, and economic impact assessments. They are designed to fill the gap for comprehensive, accurate, and accessible real estate data for researchers, policymakers, and the general public interested in the housing market dynamics.

2. *Who created the dataset (for example, which team, research group) and on behalf of which entity (for example, company, institution, organization)?*
    - The datasets available on the research page of Zillow, a leading real estate and rental marketplace, are typically created by Zillow's own economic research team. This team focuses on providing insights into the housing market and economic trends.

3. *Who funded the creation of the dataset? If there is an associated grant, please provide the name of the grantor and the grant name and number.*
    - The datasets on Zillow's research page are generally created and funded internally by Zillow Group, Inc. itself, without specific external grants. As a commercial entity with a vested interest in real estate markets, Zillow utilizes its resources to compile and analyze these datasets for public and internal use.

4. *Any other comments?*
    - None.



**Composition**

1. *What do the instances that comprise the dataset represent (for example, documents, photos, people, countries)? Are there multiple types of instances (for example, movies, users, and ratings; people and interactions between them; nodes and edges)? Please provide a description.*
	- In the dataset used for this paper, each instance representing a geographic region. These regions mainly include the country as a whole (United States), metropolitan statistical areas (e.g., New York, NY; Los Angeles, CA). The dataset contains a series of monthly average house prices, spanning from January 2000 to February 2024, as shown by the date-formatted columns. The data does not mix different types of instances (like movies, users, and ratings or people and interactions between them) but focuses solely on regional housing price data over time. Each row includes identifiers and names for the regions, the type of region (such as 'country' or 'msa' for metropolitan statistical area), and state names for regions within specific states.

2. *How many instances are there in total (of each type, if appropriate)?*
	- 50 States
	- 895 Regions
	- 290 Dates

3. *Does the dataset contain all possible instances or is it a sample (not necessarily random) of instances from a larger set? If the dataset is a sample, then what is the larger set? Is the sample representative of the larger set (for example, geographic coverage)? If so, please describe how this representativeness was validated/verified. If it is not representative of the larger set, please describe why not (for example, to cover a more diverse range of instances, because instances were withheld or unavailable).*
	- The dataset is likely a sample from a larger set of all possible geographic areas in the United States for which housing data could be collected. This larger set would include a comprehensive collection of all geographic regions in the United States, encompassing every city, town, rural area, and Metropolitan Statistical Area (MSA) with available housing market data. A clear example indicating that this is just a sample is the absence of data for Hawaii. Regarding representativeness, this data is representative, mainly because Zillow is one of the leading real estate trading companies in the United States, and its data covers almost all cities. Additionally, there is no special focus on regions that provide the most insights into the national housing market trends.

4. *What data does each instance consist of? "Raw" data or features? In either case, please provide a description.*
	- RegionID: A unique identifier for each geographic region.
  - SizeRank: A rank based on the size or significance of the region, presumably in terms of population or housing market activity.
  - RegionName: The name of the region, which can be a country (like the United States), a metropolitan statistical area (MSA), or possibly other region types.
  - RegionType: The type of region, such as 'country' or 'msa', indicating the scope or level of the geographic area.
  - StateName: The name of the state in which the region is located, applicable to regions within specific states.
  - Monthly Average House Prices: A series of columns representing the average house prices for each month, spanning from January 2000 to February 2024. These columns are named by date (e.g., '2000-01-31', '2000-02-29', etc.), with each column representing the average housing price in that region for the given month.

5. *Is there a label or target associated with each instance? If so, please provide a description.*
	- RegionID: Unique ID associated with each region in RegionName.

6. *Is any information missing from individual instances? If so, please provide a description, explaining why this information is missing (for example, because it was unavailable). This does not include intentionally removed information, but might include, for example, redacted text.*
	- Information such as the latitude and longitude for each region is not included. The absence of geographic coordinates (latitude and longitude) means that while the dataset provides a comprehensive temporal view of housing price trends across different regions, it does not directly offer spatial data that would allow for mapping or spatial analysis of these trends. The reason for missing information is because of the dataset's primary focus on economic trends and prioritizes time-series analysis of housing prices.

7. *Are relationships between individual instances made explicit? If so, please describe how these relationships are made explicit.*
	- The relationships between individual instances (regions) are not explicitly defined in terms of direct interactions or connections between them. The dataset primarily focuses on time-series data for housing prices within various geographic regions without detailing explicit relationships like proximity, hierarchical structures (e.g., how states are composed of multiple cities), or economic interdependencies among these regions.

8. *Are there recommended data splits (for example, training, development/validation, testing)? If so, please provide a description of these splits, explaining the rationale behind them.*
	- There are detailed datasets for houses with specific number of bedrooms. Thus no need to split anything.

9. *Are there any errors, sources of noise, or redundancies in the dataset? If so, please provide a description.*
	- None.

10. *Is the dataset self-contained, or does it link to or otherwise rely on external resources? If it links to or relies on external resources, a) are there guarantees that they will exist, and remain constant, over time; b) are there official archival versions of the complete dataset (that is, including the external resources as they existed at the time the dataset was created); c) are there any restrictions (for example, licenses, fees) associated with any of the external resources that might apply to a dataset consumer? Please provide descriptions of all external resources and any restrictions associated with them, as well as links or other access points, as appropriate.*
	- This is a self-contained dataset.

11. *Does the dataset contain data that might be considered confidential (for example, data that is protected by legal privilege or by doctor-patient confidentiality, data that includes the content of individuals' non-public communications)? If so, please provide a description.*
	- None.

12. *Does the dataset contain data that, if viewed directly, might be offensive, insulting, threatening, or might otherwise cause anxiety? If so, please describe why.*
	- None.

13. *Does the dataset identify any sub-populations (for example, by age, gender)? If so, please describe how these subpopulations are identified.*
	- The data are labled with StateName and RegionName. Therefore, data from each state or region coule be view as a subpopulations of this dataset.

14. *Is it possible to identify individuals (that is, one or more natural persons), either directly or indirectly (that is, in combination with other data) from the dataset? If so, please describe how.*
	- None.

15. *Does the dataset contain data that might be considered sensitive in any way (for example, data that reveals race or ethnic origins, sexual orientations, religious beliefs, political opinions or union memberships, or locations; financial or health data; biometric or genetic data; forms of government identification, such as social security numbers; criminal history)? If so, please provide a description.*
	- None.

16. *Any other comments?*
	- None.



**Collection process**

1. *How was the data associated with each instance acquired? Was the data directly observable (for example, raw text, movie ratings), reported by subjects (for example, survey responses), or indirectly inferred/derived from other data (for example, part-of-speech tags, model-based guesses for age or language)? If the data was reported by subjects or indirectly inferred/derived from other data, was the data validated/verified? If so, please describe how. How were these mechanisms or procedures validated?*
	- The data in the Zillow dataset, specifically the housing prices for various regions, is primarily derived from Zillow's own listings and transactions data, along with public records and assessments. Here's a breakdown of the data acquisition methods:
	  - Direct Observation and Public Records: Zillow aggregates housing price data from several sources, including direct listings on their platform, real estate transactions, and public property records such as sales and assessments. This means that much of the data is directly observable or comes from official records, making it a robust and reliable source of housing market information.
	  - Zestimate: Some of the housing price data, particularly for times or areas where direct transaction data may be sparse, could be supplemented by Zillow's Zestimate® home values. The Zestimate is an estimated market value calculated using proprietary models that analyze public and user-submitted data. This would fall under data indirectly inferred/derived from other data.
	  - Validation and Verification: For directly observed or recorded data (listings, transactions, public records), the validity comes from the data's official or commercial nature. These are factual records of housing sales and listings. For data like the Zestimate, Zillow continuously updates and refines its models based on new data, market trends, and feedback. The accuracy of Zestimates is evaluated by comparing estimated values with actual sale prices when they become available, and Zillow publishes accuracy metrics for its Zestimates, providing a form of validation.

2. *What mechanisms or procedures were used to collect the data (for example, hardware apparatuses or sensors, manual human curation, software programs, software APIs)?*
	- Mainly direct obervation (using data from their platform, as mentioned in previous question).

3. *If the dataset is a sample from a larger set, what was the sampling strategy (for example, deterministic, probabilistic with specific sampling probabilities)?*
	- Not mentioned by data provider.

4. *Who was involved in the data collection process (for example, students, crowdworkers, contractors) and how were they compensated (for example, how much were crowdworkers paid)?*
	- Zillow’s Platform and Automated Systems: Much of the data collection is carried out through Zillow's own technological infrastructure, which aggregates and processes listings, sales data, and property information from across the United States. This process is automated, involving sophisticated software systems designed to handle large volumes of data.
	- Real Estate Professionals: Realtors and other real estate professionals often use Zillow to list properties. While their primary motivation is to market properties to potential buyers, their contributions add to the dataset's comprehensiveness. Compensation for these professionals comes through the real estate transactions facilitated by their listings, not directly from Zillow for the data per se.

5. *Over what timeframe was the data collected? Does this timeframe match the creation timeframe of the data associated with the instances? If not, please describe the timeframe in which the data associated with the instances was created.*
	- The dataset provided spans from January 2000 to February 2024, indicating that the data collection covers this specific timeframe. This period reflects the creation timeframe of the data associated with each instance, which means each instance's housing price data was collected or estimated for these specific monthly intervals.

6. *Were any ethical review processes conducted? If so, please provide a description of these review processes, including the outcomes, as well as a link or other access point to any supporting documentation.*
	- For datasets like the one provided by Zillow, which compiles housing market data across various regions in the United States, specific ethical review processes might not be as prominently documented or required in the same manner as they would be for research involving human subjects directly. However, Zillow, like many data-providing entities, operates within a framework of legal and ethical considerations, especially regarding privacy, data accuracy, and transparency.

7. *Did you collect the data from the individuals in question directly, or obtain it via third parties or other sources (for example, websites)?*
	- Data was obtained from Zillow.

8. *Were the individuals in question notified about the data collection? If so, please describe (or show with screenshots or other information) how notice was provided, and provide a link or other access point to, or otherwise reproduce, the exact language of the notification itself.*
	- This is a publicly available information on housing prices and transactions.

9. *Has an analysis of the potential impact of the dataset and its use on data subjects (for example, a data protection impact analysis) been conducted? If so, please provide a description of this analysis, including the outcomes, as well as a link or other access point to any supporting documentation.*
	- None

10. *Any other comments?*
	- None.



**Preprocessing/cleaning/labeling**

1. *Was any preprocessing/cleaning/labeling of the data done (for example, discretization or bucketing, tokenization, part-of-speech tagging, SIFT feature extraction, removal of instances, processing of missing values)? If so, please provide a description. If not, you may skip the remaining questions in this section.*
	- Cleaning:
	  - Columns (SizeRank, RegionID, RegionType, RegionName) are removed from the dataset, likely because they are not necessary for the subsequent analysis focused on state-level price trends.
	- Further Processing:
	  - Monthly Aggregation: The data is grouped by StateName, and the mean house price for each state is calculated for each month. This step involves aggregating all numeric columns (assumed to be monthly house price data) and computing their means, excluding missing values.
	  - Yearly Aggregation: The monthly data is then transformed to calculate the average house price by year for each state. This involves converting month columns to a long format, extracting the year from each date, and then calculating the yearly mean house price for each state.
	
2. *Was the "raw" data saved in addition to the preprocessed/cleaned/labeled data (for example, to support unanticipated future uses)? If so, please provide a link or other access point to the "raw" data.*
	- The raw data was saved in the project folder [https://github.com/iJustinn/House_Price.git], specific location identified in the README section.

3. *Is the software that was used to preprocess/clean/label the data available? If so, please provide a link or other access point.*
	- Programming langugae R [https://cran.r-project.org/] in the IDE RStudio [https://www.rstudio.com/products/rstudio/download/].

4. *Any other comments?*
	- None.



**Uses**

1. *Has the dataset been used for any tasks already? If so, please provide a description.*
	- Building modeling and creating charts for this project.

2. *Is there a repository that links to any or all papers or systems that use the dataset? If so, please provide a link or other access point.*
	- GitHub Link[https://github.com/iJustinn/House_Price.git]

3. *What (other) tasks could the dataset be used for?*
	- None.

4. *Is there anything about the composition of the dataset or the way it was collected and preprocessed/cleaned/labeled that might impact future uses? For example, is there anything that a dataset consumer might need to know to avoid uses that could result in unfair treatment of individuals or groups (for example, stereotyping, quality of service issues) or other risks or harms (for example, legal risks, financial harms)? If so, please provide a description. Is there anything a dataset consumer could do to mitigate these risks or harms?*
	- The processed dataset, centered around housing prices across various states and time periods, is limited to only use for similar tasks of this project. Hence it has impact its future use.

5. *Are there tasks for which the dataset should not be used? If so, please provide a description.*
	- Legal or Regulatory Compliance: Using aggregated and possibly anonymized data for purposes requiring detailed, verified information — such as legal compliance, zoning decisions, or adherence to housing regulations — might not be appropriate. Such applications typically require specific, case-by-case data rather than broad averages or trends.
	- Short-term Investment Decisions: While historical data can highlight trends, using it for short-term real estate investment decisions without considering current market dynamics, economic indicators, and local factors could lead to misguided decisions. The dataset likely does not capture rapid market changes or short-term fluctuations.

6. *Any other comments?*
	- None.



**Distribution**

1. *Will the dataset be distributed to third parties outside of the entity (for example, company, institution, organization) on behalf of which the dataset was created? If so, please provide a description.*
	- None.

2. *How will the dataset be distributed (for example, tarball on website, API, GitHub)? Does the dataset have a digital object identifier (DOI)?*
	- GitHub, as mentioned in Uses section.

3. *When will the dataset be distributed?*
	- April, 2024. The time when this project is uploaded onto GitHub.

4. *Will the dataset be distributed under a copyright or other intellectual property (IP) license, and/or under applicable terms of use (ToU)? If so, please describe this license and/ or ToU, and provide a link or other access point to, or otherwise reproduce, any relevant licensing terms or ToU, as well as any fees associated with these restrictions.*
	- None.

5. *Have any third parties imposed IP-based or other restrictions on the data associated with the instances? If so, please describe these restrictions, and provide a link or other access point to, or otherwise reproduce, any relevant licensing terms, as well as any fees associated with these restrictions.*
	- None.

6. *Do any export controls or other regulatory restrictions apply to the dataset or to individual instances? If so, please describe these restrictions, and provide a link or other access point to, or otherwise reproduce, any supporting documentation.*
	- None.

7. *Any other comments?*
	- None.


**Maintenance**

1. *Who will be supporting/hosting/maintaining the dataset?*
	- The dataset will be held at server of GitHub, regulated by the owner of this project.

2. *How can the owner/curator/manager of the dataset be contacted (for example, email address)?*
	- The owner can be reached via GitHub account.

3. *Is there an erratum? If so, please provide a link or other access point.*
	- None.

4. *Will the dataset be updated (for example, to correct labeling errors, add new instances, delete instances)? If so, please describe how often, by whom, and how updates will be communicated to dataset consumers (for example, mailing list, GitHub)?*
	- Nothing about this project will updated after it is done.

5. *If the dataset relates to people, are there applicable limits on the retention of the data associated with the instances (for example, were the individuals in question told that their data would be retained for a fixed period of time and then deleted)? If so, please describe these limits and explain how they will be enforced.*
	- None.

6. *Will older versions of the dataset continue to be supported/hosted/maintained? If so, please describe how. If not, please describe how its obsolescence will be communicated to dataset consumers.*
	- Only datasets on the GitHub will be hosted.

7. *If others want to extend/augment/build on/contribute to the dataset, is there a mechanism for them to do so? If so, please provide a description. Will these contributions be validated/verified? If so, please describe how. If not, why not? Is there a process for communicating/distributing these contributions to dataset consumers? If so, please provide a description.*
	-  People other than the owner can extend the dataset via collaboration feature of GitHub, or directly contact owner for further work.

8. *Any other comments?*
	- None.
	


\newpage


# References


