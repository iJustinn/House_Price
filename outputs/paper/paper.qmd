---
title: "Analyzing Trends in U.S. Residential Real Estate Prices"
subtitle: "A Data-Driven Approach to Understand the Market"
author: 
  - Ziheng Zhong
thanks: "Code and data are available at: https://github.com/iJustinn/House_Price.git"
date: today
date-format: long
abstract: "This research explores the dynamics of the U.S. housing market. It provides a comprehensive analysis of price trends across different states and housing types, revealing significant escalations in recent years. The study elucidates how location, property characteristics, and external economic factors influence house prices."
format: pdf
toc: true
toc-depth: 3
number-sections: true
bibliography: references.bib
---

```{r}
#| message: false
#| echo: false

# load packages
packages <- c("tidyverse", "ggplot2", "janitor", "readr", "knitr", "modelsummary", "testthat", "kableExtra", "viridis", "lubridate", "maps", "mgcv")

# Function to check and install missing packages
install_and_load <- function(packages) {
  for (package in packages) {
    if (!require(package, character.only = TRUE)) {
      install.packages(package)
      library(package, character.only = TRUE)
    }
  }
}

# Call the function with the list of packages
install_and_load(packages)
```

```{r}
#| message: false
#| echo: false

# read in data
all_house_price <- read_csv("../../inputs/data/all_house_price.csv")
house_prices <- read_csv("../../outputs/data/merged_house_price.csv")

state_house_price_by_year <- read_csv("../../outputs/data/state_house_price_by_year.csv")
state_1b_house_price_by_year <- read_csv("../../outputs/data/state_1b_house_price_by_year.csv")
state_2b_house_price_by_year <- read_csv("../../outputs/data/state_2b_house_price_by_year.csv")
state_3b_house_price_by_year <- read_csv("../../outputs/data/state_3b_house_price_by_year.csv")
state_4b_house_price_by_year <- read_csv("../../outputs/data/state_4b_house_price_by_year.csv")
state_5bplus_house_price_by_year <- read_csv("../../outputs/data/state_5bplus_house_price_by_year.csv")

state_house_price_by_month <- read_csv("../../outputs/data/state_house_price_by_month.csv")
state_1b_house_price_by_month <- read_csv("../../outputs/data/state_1b_house_price_by_month.csv")
state_2b_house_price_by_month <- read_csv("../../outputs/data/state_2b_house_price_by_month.csv")
state_3b_house_price_by_month <- read_csv("../../outputs/data/state_3b_house_price_by_month.csv")
state_4b_house_price_by_month <- read_csv("../../outputs/data/state_4b_house_price_by_month.csv")
state_5bplus_house_price_by_month <- read_csv("../../outputs/data/state_5bplus_house_price_by_month.csv")

```

# Introduction

This paper will focus on exploring trends in U.S. house prices, a pertinent issue as real estate markets worldwide have experienced significant increases. Cities like Toronto, Canada, and Beijing, China have witnessed substantial escalations in housing costs, reflecting a global phenomenon. This study shifts its focus to the United States, one of the most economically stable and developed countries in the world, to determine if similar trends persist in its real estate market. By examining the U.S. housing market, this research aims to uncover the principal factors influencing house prices.

The U.S. real estate market offers a unique case study due to its diverse economic landscapes and varied housing markets across states and metropolitan areas. This analysis will consider variables such as location, number of bedrooms, and other key factors that are believed to significantly impact house pricing dynamics. Understanding these factors is crucial, as the findings will provide insights into how policy adjustments and economic changes can affect housing affordability. Ultimately, this research aims to serve as a guide for policymakers, investors, and the public, helping them make informed decisions regarding housing investments and urban planning.

Following this introduction, @sec-data (Methodology), outlines key reasons of the dataset choice, main processing techniques employed, emphasizing transparency and replicability. @sec-res (Results) presents the findings, specifically focusing on the dynamics of house prices across various U.S. states and metropolitan areas. @sec-dis (Discussion), analyzes these findings from the perspectives outlined earlier, integrating economic, policy, and regional variables. Finally, @sec-con (Conclusion), summarizes the key insights and implications of this research, offering recommendations for policymakers and stakeholders involved in the housing market.

# Data {#sec-data}

Data used in this paper was cleaned, processed and tested with the programming language R [@citeR]. Also with support of additional packages in R: `tidyverse` [@citeTidyverse], `ggplot2` [@citeGgplot], `janitor` [@citeJanitor], `readr` [@citeReadr], `knitr` [@citeKnitr], `modelsummary` [@citeModelsummary], `testthat` [@citeTestthat], `KableExtra` [@citeKableEx], `viridis` [@citeViridis], `lubridate` [@citeLubridate], `maps` [@citeMaps], `mgcv` [@citeMgcv].

## Source

Our study focuse on U.S. house price trends using the comprehensive datasets available from Zillow Research  [@citeZillowdata]. This data repository offers an extensive array of real estate statistics, capturing the dynamics of house prices across various regions in the United States. As a cornerstone of transparency and public engagement, Zillow Research provides open access to these crucial real estate data, which are instrumental in understanding housing market behaviors.

The dataset includes detailed information on house prices over several years, with breakdowns by state, city, and sometimes neighborhood levels, from median list prices to Zillow Home Value Index (ZHVI). This granularity allows for a nuanced analysis of house price trends and the identification of significant market dynamics, guiding potential real estate investments and policy formulations.

We selected this dataset for its reliability, depth, and alignment with our study’s goals. As a trusted source in the real estate market, it provides accurate and timely data crucial for our analysis. Its comprehensive geographic and property-specific details allow for an in-depth examination of factors influencing house prices in the U.S. Additionally, the dataset’s structure supports efficient analysis and modeling of real estate trends. More details about this dataset can be found in the datasheet in @sec-app (Appendix).

## Method

Our analysis commenced with the essential task of refining the raw dataset downloaded from Zillow. The initial phase involved understanding the dataset’s structure. As the @tbl-raw_data showing, this dataset typically consists of multiple attributes such as Region ID, Region Type, location (states), year of transaction, house price, etc. Each record represents a unique property, providing a comprehensive snapshot of its characteristics.

```{r}
#| message: false
#| echo: false
#| warning: false
#| fig-pos: 'h'
#| label: tbl-raw_data
#| tbl-cap: Raw Data Preview

kable(head(all_house_price)[, 1:7])

```

Then, we proceeded to clean and process the data as needed, setting the stage for deeper analysis. Our first step involved preprocessing the raw dataset from Zillow, which required several critical adjustments to ensure data quality and usability. Initially, we removed the first row of the dataset, which represented national averages rather than state-specific data, to focus our analysis on regional trends. This step was crucial as the inclusion of national data could skew the results when examining state-level dynamics.

Next, we streamlined the dataset by removing less relevant columns such as 'SizeRank', 'RegionID', 'RegionType', and 'RegionName'. These columns were extraneous for our analysis, which aimed to concentrate on price fluctuations and trends across different states. By simplifying the dataset to whcih showing in @tbl-clean_data, we facilitated more focused and faster computations in subsequent steps.

```{r}
#| message: false
#| echo: false
#| warning: false
#| fig-pos: 'h'
#| label: tbl-clean_data
#| tbl-cap: Clean Data Preview

clean_house_price <- all_house_price %>%
  select(-c(SizeRank, RegionID, RegionType, RegionName))

kable(head(clean_house_price)[, 1:7])

```

After the initial cleanup, we performed a more detailed grouping and summarization of the data by state on a monthly basis showing in left table of @tbl-processed. For each state, we calculated the mean house price, excluding any missing values to ensure the accuracy of our results. This aggregation provided a clearer view of the monthly pricing trends across different states, serving as a foundational analysis for identifying patterns and anomalies.

The data was further processed to derive yearly price trends from the monthly data showing in right table of @tbl-processed. We transformed the dataset by pivoting the monthly columns into a single column of house prices, tagged by corresponding months. After converting these month labels into date formats and extracting the year, we calculated the average house price per state for each year. This yearly aggregation helped in examining longer-term trends and making more strategic conclusions about the housing market dynamics.

```{r}
#| message: false
#| echo: false
#| warning: false
#| fig-pos: 'h'
#| label: tbl-processed
#| tbl-cap: Processed Data Preview (by Month & by Year)
#| layout-ncol: 2
#| layout-nrow: 1

state_house_price_by_month <- clean_house_price %>%
  group_by(StateName) %>%
  summarise(across(where(is.numeric), mean, na.rm = TRUE))

state_house_price_by_year <- state_house_price_by_month %>%
  pivot_longer(
    cols = starts_with("20"),
    names_to = "Month",
    values_to = "HousePrice"
  ) %>%
  mutate(Year = year(ymd(Month))) %>%
  group_by(StateName, Year) %>%
  summarise(AvgHousePrice = mean(HousePrice, na.rm = TRUE), .groups = 'drop')

kable(head(state_house_price_by_month)[, 1:3])

kable(head(state_house_price_by_year))

```

Finally, as @tbl-hm_data shows, to create detailed heatmaps that visualize the distribution of average house prices across the United States, we enriched our dataset with additional geographic information. The subregion column are all NA since we do not need more detail than cities. This enhancement involved integrating region-specific data, along with precise longitude and latitude coordinates for each state. By incorporating these spatial dimensions, we were able to generate a more nuanced representation of the data, enabling us to pinpoint variations in housing prices with greater accuracy.

```{r}
#| message: false
#| echo: false
#| warning: false
#| fig-pos: 'h'
#| label: tbl-hm_data
#| tbl-cap: Heatmap Data

us_states_map <- map_data("state") # Getting map data for US states

state_abbrev_to_full <- c( # Define the mapping from state abbreviations to full names
  AL = "alabama", AK = "alaska", AZ = "arizona", AR = "arkansas", CA = "california",
  CO = "colorado", CT = "connecticut", DE = "delaware", FL = "florida", GA = "georgia",
  HI = "hawaii", ID = "idaho", IL = "illinois", IN = "indiana", IA = "iowa",
  KS = "kansas", KY = "kentucky", LA = "louisiana", ME = "maine", MD = "maryland",
  MA = "massachusetts", MI = "michigan", MN = "minnesota", MS = "mississippi",
  MO = "missouri", MT = "montana", NE = "nebraska", NV = "nevada", NH = "new hampshire",
  NJ = "new jersey", NM = "new mexico", NY = "new york", NC = "north carolina",
  ND = "north dakota", OH = "ohio", OK = "oklahoma", OR = "oregon", PA = "pennsylvania",
  RI = "rhode island", SC = "south carolina", SD = "south dakota", TN = "tennessee",
  TX = "texas", UT = "utah", VT = "vermont", VA = "virginia", WA = "washington",
  WV = "west virginia", WI = "wisconsin", WY = "wyoming", DC = "district of columbia"
)

overall_hm_data <- na.omit(state_house_price_by_year)
overall_hm_data$StateName <- tolower(state_abbrev_to_full[overall_hm_data$StateName])

overall_hm_data <- overall_hm_data %>%
  group_by(StateName) %>%
  summarise(AvgHousePrice = mean(AvgHousePrice, na.rm = TRUE))

overall_hm_map_merged_data <- merge(us_states_map, overall_hm_data, by.x = "region", by.y = "StateName")

kable(head(overall_hm_map_merged_data))

```

By meticulously cleaning and structuring the data through these steps, we ensured that our dataset was not only more manageable but also primed for high-quality, reliable analysis. This thorough preparation was instrumental in supporting our subsequent charting, statistical analyses and predictive modeling efforts.

# Results {#sec-res}

```{r}
#| include: false

#### Multiple Regression Model ####
model_multi <- lm(AvgHousePrice ~ Year + NumBedroom, data = house_prices)

#### Polynomial Regression Model ####
model_poly <- lm(AvgHousePrice ~ poly(Year, 2) + poly(NumBedroom, 2), data = house_prices)

#### Generalized Additive Models ####
num_unique_years <- length(unique(house_prices$Year))
num_unique_bedrooms <- length(unique(house_prices$NumBedroom))

gam_model <- gam(AvgHousePrice ~ s(Year, k=num_unique_years-1) + s(NumBedroom, k=num_unique_bedrooms-1), data = house_prices)

```

## Data Trend

```{r}
#| message: false
#| echo: false
#| warning: false
#| fig-pos: 'h'
#| label: fig-trend
#| fig-subcap: ["Trend of Average House Price from 2000 to 2024", "Trend of Average House Price from 2000 to 2024 by House Type"]
#| layout-ncol: 1
#| layout-nrow: 2
#| fig-width: 7
#| fig-height: 4

state_all <- state_house_price_by_month %>% select(-StateName) %>%  # Processing datasets
  summarise_all(mean, na.rm = TRUE) %>%
  pivot_longer(everything(), names_to = "Date", values_to = "AverageHousePrice")

state_all$Date <- as.Date(state_all$Date) # Convert Date from character to Date type

extrema <- state_all %>%
  mutate(is_peak = ifelse(lag(AverageHousePrice, 1, default = first(AverageHousePrice)) < AverageHousePrice & 
                            lead(AverageHousePrice, 1, default = last(AverageHousePrice)) < AverageHousePrice,
                          TRUE, FALSE),
         is_trough = ifelse(lag(AverageHousePrice, 1, default = first(AverageHousePrice)) > AverageHousePrice & 
                              lead(AverageHousePrice, 1, default = last(AverageHousePrice)) > AverageHousePrice,
                            TRUE, FALSE)) %>%
  filter(is_peak | is_trough) %>%
  select(Date, AverageHousePrice, is_peak, is_trough)

extrema <- extrema %>%
  filter(format(Date, "%Y") %in% c("2007", "2012", "2022"))

ggplot(state_all, aes(x = Date, y = AverageHousePrice)) + # Plotting
  geom_line(color = "black") + 
  geom_text(data = extrema, aes(label = format(Date, "%Y")), nudge_y = 10000, check_overlap = TRUE, color = "black") +
  geom_vline(data = extrema, aes(xintercept = as.numeric(Date)), linetype = "dashed", color = "red") +
  labs(x = "Year", y = "Average House Price ($)") + 
  theme_minimal() + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

process_data <- function(data, label) {
  data %>% 
    select(-StateName) %>%
    summarise_all(mean, na.rm = TRUE) %>%
    pivot_longer(everything(), names_to = "Date", values_to = "AverageHousePrice") %>%
    mutate(Trend = label,
           Date = as.Date(Date))
}

data_list <- list(
  `1B` = state_1b_house_price_by_month,
  `2B` = state_2b_house_price_by_month,
  `3B` = state_3b_house_price_by_month,
  `4B` = state_4b_house_price_by_month,
  `5B+` = state_5bplus_house_price_by_month
)

processed_data <- lapply(names(data_list), function(name) process_data(data_list[[name]], name))

combined_data <- bind_rows(processed_data)

ggplot(combined_data, aes(x = Date, y = AverageHousePrice, color = Trend)) +
  geom_line() + 
  labs(x = "Year", y = "Average House Price ($)") +
  theme_minimal() + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_color_viridis_d(name = "Number of Bedrooms")

``` 

@fig-trend-1 illustrates the average house price trend from 2000 to 2024. This graph highlights several inflection points in housing prices over the span of nearly a quarter of a century. A significant peak occurs in 2006, followed by a notable dip culminating in a trough in 2012. Subsequently, prices climb, reaching another prominent peak in 2022. The chart clearly marks these fluctuations, drawing attention to the cyclic nature of the housing market over time. When we ignore these brief highs or lows, the overall trend of housing prices has a clear upward trend.

@fig-trend-2 segments the average house price trends from 2000 to 2024 by house type, from one-bedroom to houses with five or more bedrooms. The trajectory for each house type is distinct, with the five-plus-bedroom category reaching the highest average prices, particularly exhibiting a sharp rise after 2012. The four-bedroom category shows substantial growth, albeit less sharply than the largest homes. One and two-bedroom homes reveal a more gradual increase, suggesting a different progression in price trends compared to larger properties. The disparities among the trends of different house types highlight the varied performance within the housing market. The overall price trend of different bedroom types is also showing a clear upward trend.

## Heat Maps

The heat map in @fig-hm_overall illustrates the average house prices across different states in the United States. The color gradient represents the average house price range, with the scale on the right indicating higher values in warmer colors (yellows and reds) and lower values in cooler colors (purples). The states colored with the warmer end of the spectrum exhibit higher average house prices, whereas those in the cooler end show lower average prices. There appears to be a geographical pattern where certain regions are predominantly one color, indicating a regional similarity in housing prices. States in the central part of the map are mostly in the mid-range of the color scale, neither the highest nor the lowest in average house price. The variation in color intensity among the states suggests a diversity in average house pricing across the country. Further, in the @fig-hm_overall, we can observe that there are a few states, particularly on the east and west coasts, such as California and New York, that are shaded in these most yellowish colour. These states stand out from the rest, suggesting they have notably higher average house prices compared to states with cooler colors (purples and dark blues).

```{r}
#| message: false
#| echo: false
#| warning: false
#| fig-pos: 'h'
#| label: fig-hm_overall
#| fig-cap: Average Price by State in the US for All House Types
#| fig-width: 7
#| fig-height: 2.5

us_states_map <- map_data("state") # Getting map data for US states

state_abbrev_to_full <- c( # Define the mapping from state abbreviations to full names
  AL = "alabama", AK = "alaska", AZ = "arizona", AR = "arkansas", CA = "california",
  CO = "colorado", CT = "connecticut", DE = "delaware", FL = "florida", GA = "georgia",
  HI = "hawaii", ID = "idaho", IL = "illinois", IN = "indiana", IA = "iowa",
  KS = "kansas", KY = "kentucky", LA = "louisiana", ME = "maine", MD = "maryland",
  MA = "massachusetts", MI = "michigan", MN = "minnesota", MS = "mississippi",
  MO = "missouri", MT = "montana", NE = "nebraska", NV = "nevada", NH = "new hampshire",
  NJ = "new jersey", NM = "new mexico", NY = "new york", NC = "north carolina",
  ND = "north dakota", OH = "ohio", OK = "oklahoma", OR = "oregon", PA = "pennsylvania",
  RI = "rhode island", SC = "south carolina", SD = "south dakota", TN = "tennessee",
  TX = "texas", UT = "utah", VT = "vermont", VA = "virginia", WA = "washington",
  WV = "west virginia", WI = "wisconsin", WY = "wyoming", DC = "district of columbia"
)

overall_hm_data <- na.omit(state_house_price_by_year)
overall_hm_data$StateName <- tolower(state_abbrev_to_full[overall_hm_data$StateName])

overall_hm_data <- overall_hm_data %>%
  group_by(StateName) %>%
  summarise(AvgHousePrice = mean(AvgHousePrice, na.rm = TRUE))

overall_hm_map_merged_data <- merge(us_states_map, overall_hm_data, by.x = "region", by.y = "StateName")

ggplot() +
  geom_polygon(data = overall_hm_map_merged_data, aes(x = long, y = lat, group = group, fill = AvgHousePrice), color = "white") +
  coord_fixed(1.3) +
  scale_fill_viridis(name = "Avg House Price", option = "plasma") +
  theme_minimal() +
  theme(legend.position = "right")

```

```{r}
#| message: false
#| echo: false
#| warning: false
#| fig-pos: 'h'
#| label: fig-hm_by_type
#| fig-cap: Average Price by State in the US for Different House Types
#| fig-subcap: ["one bedroom","two bedrooms","three bedrooms","four bedrooms","five plus bedrooms"]
#| layout-ncol: 2
#| layout-nrow: 3
#| fig-width: 6.5
#| fig-height: 3

us_states_map <- map_data("state")

state_abbrev_to_full <- c( # Define the mapping from state abbreviations to full names
  AL = "alabama", AK = "alaska", AZ = "arizona", AR = "arkansas", CA = "california",
  CO = "colorado", CT = "connecticut", DE = "delaware", FL = "florida", GA = "georgia",
  HI = "hawaii", ID = "idaho", IL = "illinois", IN = "indiana", IA = "iowa",
  KS = "kansas", KY = "kentucky", LA = "louisiana", ME = "maine", MD = "maryland",
  MA = "massachusetts", MI = "michigan", MN = "minnesota", MS = "mississippi",
  MO = "missouri", MT = "montana", NE = "nebraska", NV = "nevada", NH = "new hampshire",
  NJ = "new jersey", NM = "new mexico", NY = "new york", NC = "north carolina",
  ND = "north dakota", OH = "ohio", OK = "oklahoma", OR = "oregon", PA = "pennsylvania",
  RI = "rhode island", SC = "south carolina", SD = "south dakota", TN = "tennessee",
  TX = "texas", UT = "utah", VT = "vermont", VA = "virginia", WA = "washington",
  WV = "west virginia", WI = "wisconsin", WY = "wyoming", DC = "district of columbia"
)

#### US States 1b House Price Heat Map ####
oneb_hm_data <- na.omit(state_1b_house_price_by_year)
oneb_hm_data$StateName <- tolower(state_abbrev_to_full[oneb_hm_data$StateName])

oneb_hm_data <- oneb_hm_data %>%
  group_by(StateName) %>%
  summarise(AvgHousePrice = mean(AvgHousePrice, na.rm = TRUE))

oneb_hm_map_merged_data <- merge(us_states_map, oneb_hm_data, by.x = "region", by.y = "StateName")

ggplot() +
  geom_polygon(data = oneb_hm_map_merged_data, aes(x = long, y = lat, group = group, fill = AvgHousePrice), color = "white") +
  coord_fixed(1.3) +
  scale_fill_viridis(name = "Avg House Price", option = "plasma") +
  theme_minimal() +
  theme(legend.position = "right")

#### US States 2b House Price Heat Map ####
twob_hm_data <- na.omit(state_2b_house_price_by_year) # Delete all NAs
twob_hm_data$StateName <- tolower(state_abbrev_to_full[twob_hm_data$StateName]) # Convert StateName from abbreviation to full name

twob_hm_data <- twob_hm_data %>% # Taking average for house price based on states
  group_by(StateName) %>%
  summarise(AvgHousePrice = mean(AvgHousePrice, na.rm = TRUE))

twob_hm_map_merged_data <- merge(us_states_map, twob_hm_data, by.x = "region", by.y = "StateName") # Merge data ready for chart

ggplot() + # Generating heat map
  geom_polygon(data = twob_hm_map_merged_data, aes(x = long, y = lat, group = group, fill = AvgHousePrice), color = "white") +
  coord_fixed(1.3) +
  scale_fill_viridis(name = "Avg House Price", option = "plasma") +
  theme_minimal() +
  theme(legend.position = "right")

#### US States 3b House Price Heat Map ####
threeb_hm_data <- na.omit(state_3b_house_price_by_year) # Delete all NAs
threeb_hm_data$StateName <- tolower(state_abbrev_to_full[threeb_hm_data$StateName]) # Convert StateName from abbreviation to full name

threeb_hm_data <- threeb_hm_data %>% # Taking average for house price based on states
  group_by(StateName) %>%
  summarise(AvgHousePrice = mean(AvgHousePrice, na.rm = TRUE))

threeb_hm_map_merged_data <- merge(us_states_map, threeb_hm_data, by.x = "region", by.y = "StateName") # Merge data ready for chart

ggplot() + # Generating heat map
  geom_polygon(data = threeb_hm_map_merged_data, aes(x = long, y = lat, group = group, fill = AvgHousePrice), color = "white") +
  coord_fixed(1.3) +
  scale_fill_viridis(name = "Avg House Price", option = "plasma") +
  theme_minimal() +
  theme(legend.position = "right")

#### US States 4b House Price Heat Map ####
fourb_hm_data <- na.omit(state_4b_house_price_by_year) # Delete all NAs
fourb_hm_data$StateName <- tolower(state_abbrev_to_full[fourb_hm_data$StateName]) # Convert StateName from abbreviation to full name

fourb_hm_data <- fourb_hm_data %>% # Taking average for house price based on states
  group_by(StateName) %>%
  summarise(AvgHousePrice = mean(AvgHousePrice, na.rm = TRUE))

fourb_hm_map_merged_data <- merge(us_states_map, fourb_hm_data, by.x = "region", by.y = "StateName") # Merge data ready for chart

ggplot() + # Generating heat map
  geom_polygon(data = fourb_hm_map_merged_data, aes(x = long, y = lat, group = group, fill = AvgHousePrice), color = "white") +
  coord_fixed(1.3) +
  scale_fill_viridis(name = "Avg House Price", option = "plasma") +
  theme_minimal() +
  theme(legend.position = "right")

#### US States 5b+ House Price Heat Map ####
fiveb_hm_data <- na.omit(state_5bplus_house_price_by_year) # Delete all NAs
fiveb_hm_data$StateName <- tolower(state_abbrev_to_full[fiveb_hm_data$StateName]) # Convert StateName from abbreviation to full name

fiveb_hm_data <- fiveb_hm_data %>% # Taking average for house price based on states
  group_by(StateName) %>%
  summarise(AvgHousePrice = mean(AvgHousePrice, na.rm = TRUE))

fiveb_hm_map_merged_data <- merge(us_states_map, fiveb_hm_data, by.x = "region", by.y = "StateName") # Merge data ready for chart

ggplot() + # Generating heat map
  geom_polygon(data = fiveb_hm_map_merged_data, aes(x = long, y = lat, group = group, fill = AvgHousePrice), color = "white") +
  coord_fixed(1.3) +
  scale_fill_viridis(name = "Avg House Price", option = "plasma") +
  theme_minimal() +
  theme(legend.position = "right")

```

@fig-hm_by_type comprises a collection of heatmaps (labeled a through e) that delineate the average house prices by state within the United States, segmented by the number of bedrooms. In the one-bedroom category @fig-hm_by_type-1, the color distribution is relatively homogeneous, suggesting minor variations in average prices across the states. Moving to the two-bedroom category @fig-hm_by_type-2, there is a discernible increase in the range of colors, which implies a modest expansion in the price range across different states.

As the number of bedrooms increases to three @fig-hm_by_type-3, the color contrast intensifies, indicative of a wider disparity in average prices among the states. This trend continues with the four-bedroom heatmap @fig-hm_by_type-4, where a richer variety of colors emerges, particularly with several states displaying notably darker hues, signaling higher average prices.

The heatmap for homes with five or more bedrooms @fig-hm_by_type-5 showcases the most striking contrast in color intensity, with several states marked by significantly darker shades. This pattern demonstrates that there is a broad spectrum of average prices for larger homes, with some states showing considerably higher average prices than others, as evidenced by the more intense coloring. Each heatmap distinctly represents the average price range for each house category, visually emphasizing the range from lower to higher average prices across the states.

## Modeling

### Model Setup

$$\text{Price} = \beta_0 + \beta_1 \times \text{Year} + \beta_2 \times \text{NumBedroom}$$

The setup for the Multiple Regression model is straightforward, with direct relationships between the independent variables (Year and NumBedroom) and the dependent variable (house price). This model assumes that these relationships are linear and additive.

$$\text{Price} = \beta_0 + \beta_1 \times \text{Year} + \beta_2 \times \text{Year}^2 + \beta_3 \times \text{NumBedroom} + \beta_4 \times \text{NumBedroom}^2$$

In contrast, the Polynomial Regression model requires a more intricate setup. It involves creating polynomial transformations of the independent variables to allow the model to fit non-linear trends. Specifically, the second-degree polynomial terms are created to model the curvature in the data, accounting for the possibility that the effect of time and the number of bedrooms on house prices changes at different rates. This setup acknowledges that increases in house prices may accelerate or decelerate over time and that the price premium for additional bedrooms may not be constant but could increase or decrease with the number of bedrooms.

$$\text{Intercept for GAM} = \beta_0$$

The Generalized Additive Model (GAM) represents a flexible approach to modeling complex, non-linear relationships between the dependent and independent variables. This Model does not really have a fixed equation with coefficients due to its use of smoothing functions. The setup for the GAM model in this study involves specifying the smooth functions of the predictors, year and number of bedrooms, allowing for the accommodation of non-linear trends that cannot be captured by traditional linear models. Unlike polynomial regression, which explicitly models non-linearity through polynomial terms, GAM uses smoothing functions that are fitted to the data in a non-parametric manner. This approach enables the model to adapt to the shape of the underlying trend without pre-specifying the form of the relationship, thus providing a tailored fit to the peculiarities in the data.

```{r}
#| message: false
#| echo: false
#| warning: false
#| fig-pos: 'h'
#| label: tbl-linear_model_result
#| tbl-cap: Modeling Results for Linear Models

modelsummary(
  list(
    "Multiple Regression" = model_multi,
    "Polynomial Regression" = model_poly
  )
)

```

After some understanding of model choices, @tbl-linear_model_result and @tbl-nonlinear_model_result displays the regression results, which will be explain with detials in later sections.

### Multiple Regression

$$\text{Price} = -13,969,628.019 + 6,958.769 \times \text{Year} + 59,897.327 \times \text{NumBedroom}$$

The Multiple Regression model reveals some intriguing insights about the housing market. It shows a substantial negative intercept, suggesting that starting from the model's baseline (which may be the earliest year in the data, though not directly interpretable), the house prices are initially low. The positive coefficient for 'Year' implies that there's a general trend of increasing house prices over time. Every passing year is associated with an increase in the average house price, as indicated by the coefficient of 6,958.769. Additionally, the 'NumBedroom' variable carries a positive coefficient of 59,897.327, which suggests that houses with more bedrooms tend to have higher prices. This model accounts for 44.5% of the variability in house prices (R-squared = 0.445), which is respectable but indicates that over half of the variance is explained by factors not included in the model.

### Polynomial Regression

$$\text{Price} = 212,976.031 + 3,866,485.706 \times \text{Year} + 1,601,816.793 \times \text{Year}^2$$
$$+ 6,591,537.312 \times \text{NumBedroom} + 1,365,656.076 \times \text{NumBedroom}^2$$

Switching to the Polynomial Regression model, the positive intercept is considerably high, indicating that the baseline price for houses at the starting point of the model is substantial. The inclusion of polynomial terms for 'Year' and 'NumBedroom' indicates a non-linear relationship between these variables and house prices. The positive coefficients for the polynomial terms of 'Year' suggest an initial increase in prices, but as the magnitude of the coefficients for the quadratic terms is lower, this might indicate a slowing growth rate or a potential future downturn in prices. The coefficients for 'NumBedroom' follow a similar pattern, hinting that the increase in price associated with additional bedrooms becomes less pronounced as the number of bedrooms increases. This model fits the data slightly better than the Multiple Regression model, as evidenced by a higher R-squared value of 0.479. The lower AIC and BIC values suggest it is more efficient, and the reduced RMSE indicates improved prediction accuracy.

### Generalized Additive Model (GAM)

$$\text{Intercept for GAM} = 212,976.031$$

The results of the GAM model as shown in Table 6 indicate a significant positive intercept, similar to the polynomial regression model, suggesting a high base price for the houses at the starting point. The R-squared value of 0.506 is an improvement over the linear models, explaining approximately 50.6% of the variability in house prices. This is indicative of the GAM's superior capability to capture the variance in house prices with its non-linear approach.

The AIC and BIC values are slightly lower than those in the polynomial regression model, which implies that the GAM provides a better model fit with less information loss. However, the reduction is not as substantial, suggesting that while the GAM improves upon the fit of linear models, it may not be drastically different in terms of information criteria.

The RMSE of the GAM is slightly lower than that of the multiple regression but higher than that of the polynomial regression, indicating that the model's predictions are reasonably accurate, although there might be some room for improvement. This metric confirms that while the GAM has enhanced the model's predictive capability, it does so in a way that is comparable but not necessarily superior to the polynomial approach, potentially due to the complexity and variability inherent in housing market data.

```{r}
#| message: false
#| echo: false
#| warning: false
#| fig-pos: 'h'
#| label: tbl-nonlinear_model_result
#| fig-cap: Modeling Results for Non-linear Models

modelsummary(
  list(
    "GAM Regression" = gam_model
  )
)

```

Furthermore, two charts in @fig-GAM_fit each showing the smoothed relationship between the predictor and the response variable (house price), with a solid line representing the estimated function and dashed lines indicating the confidence interval around the estimate.

The plot on the left shows the relationship between 'Year' and house prices. There’s a clear non-linear trend. The prices appear to have a cyclical pattern with a significant peak around 2006, followed by a sharp decline, which likely corresponds to the global financial crisis of 2008. After this trough, there’s a recovery with prices trending upwards again, but with a suggestion of leveling off towards the end of the period. This could indicate a stabilization in the market or potentially the beginning of another downturn.

The plot on the right shows the relationship between 'NumBedroom' and house prices. This relationship is more straightforward: as the number of bedrooms increases, so does the house price. The upward trend appears to be exponential rather than linear, suggesting that additional bedrooms have an increasingly positive effect on house prices. The confidence interval widens with the number of bedrooms, indicating more variability in the data for houses with more bedrooms, which could be due to fewer observations in the higher bedroom categories or greater variance in prices for larger homes.

Both plots reveal the flexibility of GAMs in modeling complex relationships, allowing for the identification of trends that simpler linear models could miss. The confidence intervals suggest the model is fairly confident about its estimates, especially for the 'NumBedroom' variable. However, the wider intervals at the extremes of the data range would call for cautious interpretation in those areas.

```{r}
#| message: false
#| echo: false
#| warning: false
#| fig-pos: 'h'
#| label: fig-GAM_fit
#| tbl-cap: GAM Regression Fit Result

plot(gam_model, pages=1)

```

### Further Justification

In comparing the Multiple Regression, Polynomial Regression, and Generalized Additive Model (GAM) applied to the housing market data, each model serves a distinct purpose based on the complexity of the relationships it can capture between the predictors and the house prices.

The Multiple Regression model, with its linear approach, is the most straightforward, offering a baseline model for comparison. It is the least complex and most transparent, which makes it a good starting point for analysis but also the most limited in capturing the nuanced behavior of the housing market. The Polynomial Regression model advances this analysis by introducing non-linearity through polynomial terms, providing a significantly better fit as evidenced by its improved R-squared value and lower information criteria scores. It acknowledges and captures the more intricate patterns in housing prices related to time and property features.

GAM takes flexibility a step further, not being constrained by predefined polynomial terms, which allows for an even more nuanced understanding of the data. This model adapts to the data's inherent trends through the use of smoothing functions, leading to an improved R-squared value compared to the Multiple Regression model. However, when compared to Polynomial Regression, the gains in model performance are modest. While the GAM model offers a refined approach, it does not significantly outperform the Polynomial Regression in terms of the information criteria and RMSE, suggesting that the increased complexity of GAM may not be necessary for this particular dataset.

However, the visual representation of data trends provided by @fig-GAM_fit is not just intuitively appealing but also offers practical insights that might be obscured in tabular regression outputs. For example, the cyclical pattern observed in the 'Year' graph aligns with known economic events that affected housing prices, demonstrating GAM's capacity to capture externalities and temporal dynamics beyond the reach of conventional linear models.

Ultimately, the choice between these models should be informed by the balance between the need for model simplicity and the desire to capture complex data relationships. While the Polynomial Regression strikes a balance offering a good fit without overly complicating the model, the GAM provides the most flexibility, which could be essential for more intricate datasets or where the precise nature of relationships between variables is less understood.

# Discussion {#sec-dis}

## 1



## 2



## 3



## 4



## Possible Improvements



# Conclusion {#sec-con}



\newpage

\appendix

# Appendix {#sec-app}

## Datasheet

**Motivation**

1. *For what purpose was the dataset created? Was there a specific task in mind? Was there a specific gap that needed to be filled? Please provide a description.*
    - The datasets are created to support a wide range of real estate analysis and research by Zillow Research group. These datasets typically serve purposes such as market trend analysis, price prediction, housing supply studies, and economic impact assessments. They are designed to fill the gap for comprehensive, accurate, and accessible real estate data for researchers, policymakers, and the general public interested in the housing market dynamics.

2. *Who created the dataset (for example, which team, research group) and on behalf of which entity (for example, company, institution, organization)?*
    - The datasets available on the research page of Zillow, a leading real estate and rental marketplace, are typically created by Zillow's own economic research team. This team focuses on providing insights into the housing market and economic trends.

3. *Who funded the creation of the dataset? If there is an associated grant, please provide the name of the grantor and the grant name and number.*
    - The datasets on Zillow's research page are generally created and funded internally by Zillow Group, Inc. itself, without specific external grants. As a commercial entity with a vested interest in real estate markets, Zillow utilizes its resources to compile and analyze these datasets for public and internal use.

4. *Any other comments?*
    - None.



**Composition**

1. *What do the instances that comprise the dataset represent (for example, documents, photos, people, countries)? Are there multiple types of instances (for example, movies, users, and ratings; people and interactions between them; nodes and edges)? Please provide a description.*
	- In the dataset used for this paper, each instance representing a geographic region. These regions mainly include the country as a whole (United States), metropolitan statistical areas (e.g., New York, NY; Los Angeles, CA). The dataset contains a series of monthly average house prices, spanning from January 2000 to February 2024, as shown by the date-formatted columns. The data does not mix different types of instances (like movies, users, and ratings or people and interactions between them) but focuses solely on regional housing price data over time. Each row includes identifiers and names for the regions, the type of region (such as 'country' or 'msa' for metropolitan statistical area), and state names for regions within specific states.

2. *How many instances are there in total (of each type, if appropriate)?*
	- 50 States
	- 895 Regions
	- 290 Dates

3. *Does the dataset contain all possible instances or is it a sample (not necessarily random) of instances from a larger set? If the dataset is a sample, then what is the larger set? Is the sample representative of the larger set (for example, geographic coverage)? If so, please describe how this representativeness was validated/verified. If it is not representative of the larger set, please describe why not (for example, to cover a more diverse range of instances, because instances were withheld or unavailable).*
	- The dataset is likely a sample from a larger set of all possible geographic areas in the United States for which housing data could be collected. This larger set would include a comprehensive collection of all geographic regions in the United States, encompassing every city, town, rural area, and Metropolitan Statistical Area (MSA) with available housing market data. A clear example indicating that this is just a sample is the absence of data for Hawaii. Regarding representativeness, this data is representative, mainly because Zillow is one of the leading real estate trading companies in the United States, and its data covers almost all cities. Additionally, there is no special focus on regions that provide the most insights into the national housing market trends.

4. *What data does each instance consist of? "Raw" data or features? In either case, please provide a description.*
	- RegionID: A unique identifier for each geographic region.
  - SizeRank: A rank based on the size or significance of the region, presumably in terms of population or housing market activity.
  - RegionName: The name of the region, which can be a country (like the United States), a metropolitan statistical area (MSA), or possibly other region types.
  - RegionType: The type of region, such as 'country' or 'msa', indicating the scope or level of the geographic area.
  - StateName: The name of the state in which the region is located, applicable to regions within specific states.
  - Monthly Average House Prices: A series of columns representing the average house prices for each month, spanning from January 2000 to February 2024. These columns are named by date (e.g., '2000-01-31', '2000-02-29', etc.), with each column representing the average housing price in that region for the given month.

5. *Is there a label or target associated with each instance? If so, please provide a description.*
	- RegionID: Unique ID associated with each region in RegionName.

6. *Is any information missing from individual instances? If so, please provide a description, explaining why this information is missing (for example, because it was unavailable). This does not include intentionally removed information, but might include, for example, redacted text.*
	- Information such as the latitude and longitude for each region is not included. The absence of geographic coordinates (latitude and longitude) means that while the dataset provides a comprehensive temporal view of housing price trends across different regions, it does not directly offer spatial data that would allow for mapping or spatial analysis of these trends. The reason for missing information is because of the dataset's primary focus on economic trends and prioritizes time-series analysis of housing prices.

7. *Are relationships between individual instances made explicit? If so, please describe how these relationships are made explicit.*
	- The relationships between individual instances (regions) are not explicitly defined in terms of direct interactions or connections between them. The dataset primarily focuses on time-series data for housing prices within various geographic regions without detailing explicit relationships like proximity, hierarchical structures (e.g., how states are composed of multiple cities), or economic interdependencies among these regions.

8. *Are there recommended data splits (for example, training, development/validation, testing)? If so, please provide a description of these splits, explaining the rationale behind them.*
	- There are detailed datasets for houses with specific number of bedrooms. Thus no need to split anything.

9. *Are there any errors, sources of noise, or redundancies in the dataset? If so, please provide a description.*
	- None.

10. *Is the dataset self-contained, or does it link to or otherwise rely on external resources? If it links to or relies on external resources, a) are there guarantees that they will exist, and remain constant, over time; b) are there official archival versions of the complete dataset (that is, including the external resources as they existed at the time the dataset was created); c) are there any restrictions (for example, licenses, fees) associated with any of the external resources that might apply to a dataset consumer? Please provide descriptions of all external resources and any restrictions associated with them, as well as links or other access points, as appropriate.*
	- This is a self-contained dataset.

11. *Does the dataset contain data that might be considered confidential (for example, data that is protected by legal privilege or by doctor-patient confidentiality, data that includes the content of individuals' non-public communications)? If so, please provide a description.*
	- None.

12. *Does the dataset contain data that, if viewed directly, might be offensive, insulting, threatening, or might otherwise cause anxiety? If so, please describe why.*
	- None.

13. *Does the dataset identify any sub-populations (for example, by age, gender)? If so, please describe how these subpopulations are identified.*
	- The data are labled with StateName and RegionName. Therefore, data from each state or region coule be view as a subpopulations of this dataset.

14. *Is it possible to identify individuals (that is, one or more natural persons), either directly or indirectly (that is, in combination with other data) from the dataset? If so, please describe how.*
	- None.

15. *Does the dataset contain data that might be considered sensitive in any way (for example, data that reveals race or ethnic origins, sexual orientations, religious beliefs, political opinions or union memberships, or locations; financial or health data; biometric or genetic data; forms of government identification, such as social security numbers; criminal history)? If so, please provide a description.*
	- None.

16. *Any other comments?*
	- None.



**Collection process**

1. *How was the data associated with each instance acquired? Was the data directly observable (for example, raw text, movie ratings), reported by subjects (for example, survey responses), or indirectly inferred/derived from other data (for example, part-of-speech tags, model-based guesses for age or language)? If the data was reported by subjects or indirectly inferred/derived from other data, was the data validated/verified? If so, please describe how. How were these mechanisms or procedures validated?*
	- The data in the Zillow dataset, specifically the housing prices for various regions, is primarily derived from Zillow's own listings and transactions data, along with public records and assessments. Here's a breakdown of the data acquisition methods:
	  - Direct Observation and Public Records: Zillow aggregates housing price data from several sources, including direct listings on their platform, real estate transactions, and public property records such as sales and assessments. This means that much of the data is directly observable or comes from official records, making it a robust and reliable source of housing market information.
	  - Zestimate: Some of the housing price data, particularly for times or areas where direct transaction data may be sparse, could be supplemented by Zillow's Zestimate® home values. The Zestimate is an estimated market value calculated using proprietary models that analyze public and user-submitted data. This would fall under data indirectly inferred/derived from other data.
	  - Validation and Verification: For directly observed or recorded data (listings, transactions, public records), the validity comes from the data's official or commercial nature. These are factual records of housing sales and listings. For data like the Zestimate, Zillow continuously updates and refines its models based on new data, market trends, and feedback. The accuracy of Zestimates is evaluated by comparing estimated values with actual sale prices when they become available, and Zillow publishes accuracy metrics for its Zestimates, providing a form of validation.

2. *What mechanisms or procedures were used to collect the data (for example, hardware apparatuses or sensors, manual human curation, software programs, software APIs)?*
	- Mainly direct obervation (using data from their platform, as mentioned in previous question).

3. *If the dataset is a sample from a larger set, what was the sampling strategy (for example, deterministic, probabilistic with specific sampling probabilities)?*
	- Not mentioned by data provider.

4. *Who was involved in the data collection process (for example, students, crowdworkers, contractors) and how were they compensated (for example, how much were crowdworkers paid)?*
	- Zillow’s Platform and Automated Systems: Much of the data collection is carried out through Zillow's own technological infrastructure, which aggregates and processes listings, sales data, and property information from across the United States. This process is automated, involving sophisticated software systems designed to handle large volumes of data.
	- Real Estate Professionals: Realtors and other real estate professionals often use Zillow to list properties. While their primary motivation is to market properties to potential buyers, their contributions add to the dataset's comprehensiveness. Compensation for these professionals comes through the real estate transactions facilitated by their listings, not directly from Zillow for the data per se.

5. *Over what timeframe was the data collected? Does this timeframe match the creation timeframe of the data associated with the instances? If not, please describe the timeframe in which the data associated with the instances was created.*
	- The dataset provided spans from January 2000 to February 2024, indicating that the data collection covers this specific timeframe. This period reflects the creation timeframe of the data associated with each instance, which means each instance's housing price data was collected or estimated for these specific monthly intervals.

6. *Were any ethical review processes conducted? If so, please provide a description of these review processes, including the outcomes, as well as a link or other access point to any supporting documentation.*
	- For datasets like the one provided by Zillow, which compiles housing market data across various regions in the United States, specific ethical review processes might not be as prominently documented or required in the same manner as they would be for research involving human subjects directly. However, Zillow, like many data-providing entities, operates within a framework of legal and ethical considerations, especially regarding privacy, data accuracy, and transparency.

7. *Did you collect the data from the individuals in question directly, or obtain it via third parties or other sources (for example, websites)?*
	- Data was obtained from Zillow.

8. *Were the individuals in question notified about the data collection? If so, please describe (or show with screenshots or other information) how notice was provided, and provide a link or other access point to, or otherwise reproduce, the exact language of the notification itself.*
	- This is a publicly available information on housing prices and transactions.

9. *Has an analysis of the potential impact of the dataset and its use on data subjects (for example, a data protection impact analysis) been conducted? If so, please provide a description of this analysis, including the outcomes, as well as a link or other access point to any supporting documentation.*
	- None

10. *Any other comments?*
	- None.



**Preprocessing/cleaning/labeling**

1. *Was any preprocessing/cleaning/labeling of the data done (for example, discretization or bucketing, tokenization, part-of-speech tagging, SIFT feature extraction, removal of instances, processing of missing values)? If so, please provide a description. If not, you may skip the remaining questions in this section.*
	- Cleaning:
	  - Columns (SizeRank, RegionID, RegionType, RegionName) are removed from the dataset, likely because they are not necessary for the subsequent analysis focused on state-level price trends.
	- Further Processing:
	  - Monthly Aggregation: The data is grouped by StateName, and the mean house price for each state is calculated for each month. This step involves aggregating all numeric columns (assumed to be monthly house price data) and computing their means, excluding missing values.
	  - Yearly Aggregation: The monthly data is then transformed to calculate the average house price by year for each state. This involves converting month columns to a long format, extracting the year from each date, and then calculating the yearly mean house price for each state.
	
2. *Was the "raw" data saved in addition to the preprocessed/cleaned/labeled data (for example, to support unanticipated future uses)? If so, please provide a link or other access point to the "raw" data.*
	- The raw data was saved in the project folder [https://github.com/iJustinn/House_Price.git], specific location identified in the README section.

3. *Is the software that was used to preprocess/clean/label the data available? If so, please provide a link or other access point.*
	- Programming langugae R [https://cran.r-project.org/] in the IDE RStudio [https://www.rstudio.com/products/rstudio/download/].

4. *Any other comments?*
	- None.



**Uses**

1. *Has the dataset been used for any tasks already? If so, please provide a description.*
	- Building modeling and creating charts for this project.

2. *Is there a repository that links to any or all papers or systems that use the dataset? If so, please provide a link or other access point.*
	- GitHub Link[https://github.com/iJustinn/House_Price.git]

3. *What (other) tasks could the dataset be used for?*
	- None.

4. *Is there anything about the composition of the dataset or the way it was collected and preprocessed/cleaned/labeled that might impact future uses? For example, is there anything that a dataset consumer might need to know to avoid uses that could result in unfair treatment of individuals or groups (for example, stereotyping, quality of service issues) or other risks or harms (for example, legal risks, financial harms)? If so, please provide a description. Is there anything a dataset consumer could do to mitigate these risks or harms?*
	- The processed dataset, centered around housing prices across various states and time periods, is limited to only use for similar tasks of this project. Hence it has impact its future use.

5. *Are there tasks for which the dataset should not be used? If so, please provide a description.*
	- Legal or Regulatory Compliance: Using aggregated and possibly anonymized data for purposes requiring detailed, verified information — such as legal compliance, zoning decisions, or adherence to housing regulations — might not be appropriate. Such applications typically require specific, case-by-case data rather than broad averages or trends.
	- Short-term Investment Decisions: While historical data can highlight trends, using it for short-term real estate investment decisions without considering current market dynamics, economic indicators, and local factors could lead to misguided decisions. The dataset likely does not capture rapid market changes or short-term fluctuations.

6. *Any other comments?*
	- None.



**Distribution**

1. *Will the dataset be distributed to third parties outside of the entity (for example, company, institution, organization) on behalf of which the dataset was created? If so, please provide a description.*
	- None.

2. *How will the dataset be distributed (for example, tarball on website, API, GitHub)? Does the dataset have a digital object identifier (DOI)?*
	- GitHub, as mentioned in Uses section.

3. *When will the dataset be distributed?*
	- April, 2024. The time when this project is uploaded onto GitHub.

4. *Will the dataset be distributed under a copyright or other intellectual property (IP) license, and/or under applicable terms of use (ToU)? If so, please describe this license and/ or ToU, and provide a link or other access point to, or otherwise reproduce, any relevant licensing terms or ToU, as well as any fees associated with these restrictions.*
	- None.

5. *Have any third parties imposed IP-based or other restrictions on the data associated with the instances? If so, please describe these restrictions, and provide a link or other access point to, or otherwise reproduce, any relevant licensing terms, as well as any fees associated with these restrictions.*
	- None.

6. *Do any export controls or other regulatory restrictions apply to the dataset or to individual instances? If so, please describe these restrictions, and provide a link or other access point to, or otherwise reproduce, any supporting documentation.*
	- None.

7. *Any other comments?*
	- None.


**Maintenance**

1. *Who will be supporting/hosting/maintaining the dataset?*
	- The dataset will be held at server of GitHub, regulated by the owner of this project.

2. *How can the owner/curator/manager of the dataset be contacted (for example, email address)?*
	- The owner can be reached via GitHub account.

3. *Is there an erratum? If so, please provide a link or other access point.*
	- None.

4. *Will the dataset be updated (for example, to correct labeling errors, add new instances, delete instances)? If so, please describe how often, by whom, and how updates will be communicated to dataset consumers (for example, mailing list, GitHub)?*
	- Nothing about this project will updated after it is done.

5. *If the dataset relates to people, are there applicable limits on the retention of the data associated with the instances (for example, were the individuals in question told that their data would be retained for a fixed period of time and then deleted)? If so, please describe these limits and explain how they will be enforced.*
	- None.

6. *Will older versions of the dataset continue to be supported/hosted/maintained? If so, please describe how. If not, please describe how its obsolescence will be communicated to dataset consumers.*
	- Only datasets on the GitHub will be hosted.

7. *If others want to extend/augment/build on/contribute to the dataset, is there a mechanism for them to do so? If so, please provide a description. Will these contributions be validated/verified? If so, please describe how. If not, why not? Is there a process for communicating/distributing these contributions to dataset consumers? If so, please provide a description.*
	-  People other than the owner can extend the dataset via collaboration feature of GitHub, or directly contact owner for further work.

8. *Any other comments?*
	- None.
	


\newpage


# References


